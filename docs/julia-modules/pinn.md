# PINN.jl

Core PINN implementation for learning power series coefficients.

**Location:** `modelcode/PINN.jl`

---

## PINNSettings Struct

```julia
struct PINNSettings
    neuron_num::Int              # Neurons per hidden layer
    seed::Int                    # Random seed
    ode_matrices::Dict{Any,Any}  # ODE coefficient matrices
    maxiters_adam::Int           # Adam iterations
    n_terms_for_power_series::Int # Degree N of power series
    num_supervised::Int          # Coefficients for supervision
    num_points::Int              # Collocation points
    x_left::Float32              # Domain left boundary
    x_right::Float32             # Domain right boundary
    supervised_weight::Float32   # Weight for supervised loss
    bc_weight::Float32           # Weight for BC loss
    pde_weight::Float32          # Weight for PDE loss
    xs::Any                      # Collocation point locations
end
```

---

## Key Functions

### `initialize_network(settings; use_gpu=false)`

Creates neural network architecture and optionally transfers parameters to GPU.

```julia
initialize_network(settings::PINNSettings; use_gpu::Bool=false) → (network, params, state)
```

**Architecture:**
- 4 hidden layers with configurable neuron count
- Sigmoid activation function
- Output layer sized for power series coefficients

When `use_gpu=true`, parameters are transferred to GPU via `CUDA.cu()`.

---

### `loss_fn(p_net, data, coeff_net, st, ode_matrix_flat, boundary_condition, settings, use_gpu=false)`

Computes loss for a single ODE. Transfers all inputs to the correct device (GPU/CPU) before the forward pass.

```julia
loss_fn(...) → (total_loss, loss_bc, loss_pde, loss_supervised)
```

**Components:**
- PDE residual loss (vectorized matrix multiply, GPU-compatible)
- Boundary condition loss (dot products with precomputed power vectors)
- Supervised coefficient loss (padded mask to avoid scalar indexing)

---

### `global_loss(p_net, settings, coeff_net, st)`

Aggregates loss across all training examples.

```julia
global_loss(...) → (mean_loss, state, aggregated_components)
```

---

### `train_pinn(settings, csv_file)`

Auto-detects GPU and trains with Adam. LBFGS is available but currently disabled pending convergence investigation. Returns CPU parameters regardless of training device.

```julia
train_pinn(settings::PINNSettings, csv_file) → (trained_params, network, state)
```

**Behavior:**
- Checks `GPUUtils.is_gpu_available()` at start
- Transfers network parameters to GPU if available
- Runs Adam optimization for `maxiters_lbfgs` iterations
- LBFGS code is present but commented out (needs further work on convergence)
- Transfers trained parameters back to CPU before returning

---

### `evaluate_solution(settings, p_trained, coeff_net, st, benchmark_dataset, data_directories)`

Evaluates trained model and generates plots.

```julia
evaluate_solution(...) → nothing
```

**Outputs:**
- Solution comparison plot
- Coefficient comparison plot
- Error analysis plot

---

## Example Usage

```julia
settings = PINNSettings(
    neuron_num = 50,
    seed = 42,
    ode_matrices = training_data,
    maxiters_adam = 10000,
    n_terms_for_power_series = 10,
    num_supervised = 5,
    num_points = 20,
    x_left = 0.0f0,
    x_right = 1.0f0,
    supervised_weight = 0.1f0,
    bc_weight = 1.0f0,
    pde_weight = 1.0f0,
    xs = collect(range(0, 1, 20))
)

p_trained, net, st = train_pinn(settings, "output.csv")
```

---

*See also: [Training Workflow](../concepts/training-workflow.md)*
